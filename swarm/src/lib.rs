// Copyright 2020 Netwarps Ltd.
//
// Permission is hereby granted, free of charge, to any person obtaining a
// copy of this software and associated documentation files (the "Software"),
// to deal in the Software without restriction, including without limitation
// the rights to use, copy, modify, merge, publish, distribute, sublicense,
// and/or sell copies of the Software, and to permit persons to whom the
// Software is furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
// FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
// DEALINGS IN THE SOFTWARE.

//! High level manager of the network.
//!
//! A [`Swarm`] contains the state of the network as a whole. The entire
//! behaviour of a libp2p network can be controlled through the `Swarm`.
//! The `Swarm` struct contains all established connections to remotes and
//! manages the state of all the substreams that have been opened, and all
//! the upgrades that were built upon these substreams.
//!
//! # Initializing a Swarm
//!
//! Creating a `Swarm` requires three things:
//!
//!  1. A network identity of the local node in form of a [`PeerId`].
//!  2. One or more implementations of the [`Transport`] trait. This is the
//!     type that will be used in order to reach nodes on the network based
//!     on their address. See the `transport` module for more information.
//!  3. One or more implementations of the [`ProtocolHandler`] trait. This is
//!     the protocols that `Swarm` is going to support.
//!
//! # Protocol Handler
//!
//! The [`ProtocolHandler`] trait defines how each active connection to a
//! remote should behave: how to handle incoming substreams, which protocols
//! are supported, etc.
//!

mod connection;
mod control;
//mod dial;
mod muxer;
mod network;
mod registry;

pub mod identify;
pub mod ping;
pub mod protocol_handler;
pub mod substream;

pub use control::Control;
pub use protocol_handler::DummyProtocolHandler;

use async_std::task;
use fnv::FnvHashMap;
use futures::channel::{mpsc, oneshot};
use futures::future::Either;
use futures::prelude::*;
use smallvec::SmallVec;
use std::collections::HashSet;
use std::time::Duration;
use std::{error, fmt};

use libp2prs_core::peerstore::PeerStore;
use libp2prs_core::upgrade::ProtocolName;
use libp2prs_core::{
    multiaddr::{protocol, Multiaddr},
    muxing::IStreamMuxer,
    transport::{upgrade::ITransportEx, TransportError},
    PeerId,
};

use crate::connection::{Connection, ConnectionId, ConnectionLimit, Direction};
use crate::control::SwarmControlCmd;
use crate::identify::{IdentifyConfig, IdentifyHandler, IdentifyInfo, IdentifyPushHandler};
use crate::muxer::Muxer;
use crate::network::NetworkInfo;
use crate::ping::{PingConfig, PingHandler};
use crate::protocol_handler::IProtocolHandler;
use crate::registry::Addresses;
use crate::substream::{StreamId, Substream};

type Result<T> = std::result::Result<T, SwarmError>;

/// Event generated by the `Swarm`.
#[derive(Debug)]
pub enum SwarmEvent {
    /// A connection to the given peer has been opened.
    ConnectionEstablished {
        /// The connection, stream muxer
        stream_muxer: IStreamMuxer,
        /// Direction of the connection
        direction: Direction,
    },
    /// A connection with the given peer has been closed.
    ConnectionClosed {
        /// The connection Id of the sub stream.
        cid: ConnectionId,
        /// The cause of the error.
        error: TransportError,
    },
    /// A stream failed to negotiate protocol with peer.
    StreamError {
        /// The connection Id of the sub stream.
        cid: ConnectionId,
        /// The cause of the error.
        error: TransportError,
    },
    /// A new substream opened.
    StreamOpened {
        // /// The sub stream.
        sub_stream: Substream,
    },
    /// A substream has been closed.
    StreamClosed {
        /// The connection Id of the sub stream.
        cid: ConnectionId,
        /// The substream Id.
        sid: StreamId,
    },
    /// An error happened on a connection during its initial handshake.
    ///
    /// This can include, for example, an error during the handshake of the encryption layer, or
    /// the connection unexpectedly closed.
    IncomingConnectionError {
        /// The remote multiaddr.
        remote_addr: Multiaddr,
        // The error that happened when listening.
        error: TransportError,
    },
    OutgoingConnectionError {
        /// The remote Peer Id.
        peer_id: PeerId,
        /// The remote multiaddr.
        remote_addr: Multiaddr,
        /// The error that happened when dialing.
        error: TransportError,
    },
    /// We connected to a peer, but we immediately closed the connection because that peer is banned.
    BannedPeer {
        /// Identity of the banned peer.
        peer_id: PeerId,
    },
    /// Tried to dial an address but it ended up being unreachaable.
    UnreachableAddr {
        /// `PeerId` that we were trying to reach.
        peer_id: PeerId,
        /// Address that we failed to reach.
        address: Multiaddr,
        // /// Error that has been encountered.
        // error: PendingConnectionError<io::Error>,
        /// Number of remaining connection attempts that are being tried for this peer.
        attempts_remaining: u32,
    },
    /// Tried to dial an address but it ended up being unreachaable.
    /// Contrary to `UnreachableAddr`, we don't know the identity of the peer that we were trying
    /// to reach.
    UnknownPeerUnreachableAddr {
        /// Address that we failed to reach.
        address: Multiaddr,
        // /// Error that has been encountered.
        // error: PendingConnectionError<io::Error>,
    },
    /// One of our listeners has reported a new local listening address.
    NewListenAddr(Multiaddr),
    /// One of our listeners has reported the expiration of a listening address.
    ExpiredListenAddr(Multiaddr),
    /// One of the listeners gracefully closed.
    ListenerClosed {
        /// The addresses that the listener was listening on. These addresses are now considered
        /// expired, similar to if a [`ExpiredListenAddr`](SwarmEvent::ExpiredListenAddr) event
        /// has been generated for each of them.
        addresses: Vec<Multiaddr>,
        /// Reason for the closure. Contains `Ok(())` if the stream produced `None`, or `Err`
        /// if the stream produced an error.
        reason: TransportError,
    },
    /// A new dialing attempt has been initiated.
    ///
    /// A [`ConnectionEstablished`](SwarmEvent::ConnectionEstablished)
    /// event is reported if the dialing attempt succeeds, otherwise a
    /// [`UnreachableAddr`](SwarmEvent::UnreachableAddr) event is reported
    /// with `attempts_remaining` equal to 0.
    Dialing(PeerId),

    /// Test only
    Testing(u32),

    /// A Ping result generated by a connection.
    PingResult {
        /// The connection Id.
        cid: ConnectionId,
        /// The result.
        /// Duration means the TTL when succeeded, or SwarmError for failed.
        result: Result<Duration>,
    },
    IdentifyResult {
        /// The connection Id.
        cid: ConnectionId,
        /// The result.
        /// Duration means the TTL when succeeded, or SwarmError for failed.
        result: Result<(IdentifyInfo, Multiaddr)>,
    },
}

type ProtocolId = &'static [u8];

/// Contains the state of the network, plus the way it should behave.
pub struct Swarm {
    /// TODO: to improve peerstore... we don't want to leak PeerStore
    pub peers: PeerStore,

    /// The protocol multistream selector.
    muxer: Muxer,

    /// The Transports.
    transports: FnvHashMap<u32, ITransportEx>,

    /// The local peer ID.
    local_peer_id: PeerId,

    /// The next listener ID to assign.
    next_connection_id: usize,

    /// List of multiaddresses we're listening on.
    listened_addrs: SmallVec<[Multiaddr; 8]>,

    /// List of multiaddresses we're listening on, after account for external IP addresses and
    /// similar mechanisms.
    external_addrs: Addresses,

    /// List of nodes for which we deny any incoming connection.
    banned_peers: HashSet<PeerId>,

    /// The all connections_by_peer connections organized by their Ids
    connections_by_id: FnvHashMap<ConnectionId, Connection>,
    /// The all connections_by_peer connections by  peer Id
    /// There might be more than one connections for a remote peer
    connections_by_peer: FnvHashMap<PeerId, Vec<ConnectionId>>,

    /// Swarm Ping service config, optional.
    /// Ping service will be started as long as a new connection is established, if enabled.
    /// The connection will be closed if Ping failure reaches the maxmium failure counts.
    ping: Option<PingConfig>,
    /// Swarm Identify service config, optional.
    /// Identify service will be started as long as a new connection is established, if enabled.
    identify: Option<IdentifyConfig>,

    //
    // /// Pending event to be delivered to connection handlers
    // /// (or dropped if the peer disconnected) before the `behaviour`
    // /// can be polled again.
    // pending_event: Option<(PeerId, PendingNotifyHandler, TInEvent)>
    /// Swarm will listen on this channel, waiting for events generated from underlying transport
    event_receiver: mpsc::UnboundedReceiver<SwarmEvent>,
    /// The Swarm event sender wil be cloned and then taken by underlying parts
    event_sender: mpsc::UnboundedSender<SwarmEvent>,

    /// Swarm will listen on this channel, for external control commands
    ctrl_receiver: mpsc::Receiver<SwarmControlCmd>,
    /// The Swarm event sender wil be cloned and then taken by others
    ctrl_sender: mpsc::Sender<SwarmControlCmd>,
}

// impl<TBehaviour, TInEvent, TOutEvent, THandler, TConnInfo> Unpin for
//     Swarm<TBehaviour, TInEvent, TOutEvent, THandler, TConnInfo>
// where
//     THandler: IntoProtocolsHandler,
//     TConnInfo: ConnectionInfo<PeerId = PeerId>,
// {
// }
#[allow(dead_code)]
impl Swarm {
    /// Builds a new `Swarm`.
    pub fn new(
        local_peer_id: PeerId,
        //_config: NetworkConfig,
    ) -> Self {
        // unbounded channel for events, so that we can send a message to ourselves
        let (event_tx, event_rx) = mpsc::unbounded();
        let (ctrl_tx, ctrl_rx) = mpsc::channel(0);
        Swarm {
            peers: PeerStore::default(),
            muxer: Muxer::new(),
            transports: Default::default(),
            local_peer_id,
            // listeners: SmallVec::with_capacity(16),
            next_connection_id: 0,
            listened_addrs: Default::default(),
            external_addrs: Default::default(),
            banned_peers: Default::default(),
            connections_by_id: Default::default(),
            connections_by_peer: Default::default(),
            ping: None,
            identify: None,
            event_receiver: event_rx,
            event_sender: event_tx,
            ctrl_receiver: ctrl_rx,
            ctrl_sender: ctrl_tx,
        }
    }

    fn assign_cid(&mut self) -> usize {
        self.next_connection_id += 1;
        self.next_connection_id
    }
    // /// Creates Swarm with transports.(TODO: only support one Transport,so far)
    // pub fn with_transports(self, transports: Vec<ITransport<TStreamMuxer>>) -> Self {
    //     let mut me = self;
    //     for transport in transports {
    //         me = me.with_transport(transport);
    //     }
    //     me
    // }
    /// Creates Swarm with transport.
    pub fn with_transport(mut self, transport: ITransportEx) -> Self {
        let protocols = transport.protocols();
        if protocols.is_empty() {
            panic!("Shouldn't happen: no protocols found in Transport");
        }
        let mut registered: Vec<protocol::Protocol> = vec![];
        for p in protocols.iter() {
            if self.transports.get(p).is_some() {
                let proto = protocol::Protocol::get_enum(*p).unwrap();
                registered.push(proto);
            }
        }
        if !registered.is_empty() {
            log::warn!("transports already registered for protocol(s): {:?}", registered);
        }

        for p in protocols.iter() {
            log::trace!("add protocol={}", p);
            self.transports.insert(*p, transport.box_clone());
        }
        self
    }
    /// Creates Swarm with protocol handler.
    pub fn with_protocol(mut self, p: IProtocolHandler) -> Self {
        self.muxer.add_protocol_handler(p);
        self
    }
    /// Creates Swarm with Ping service.
    pub fn with_ping(mut self, ping: PingConfig) -> Self {
        self.ping = Some(ping);
        self.muxer.add_protocol_handler(Box::new(PingHandler));
        self
    }
    /// Creates Swarm with Identify service.
    pub fn with_identify(mut self, id: IdentifyConfig) -> Self {
        self.identify = Some(id);
        let handler = IdentifyHandler::new(self.ctrl_sender.clone());
        self.muxer.add_protocol_handler(Box::new(handler));
        let handler = IdentifyPushHandler::new(self.event_sender.clone());
        self.muxer.add_protocol_handler(Box::new(handler));
        self
    }
    /// Get a controller for Swarm.
    pub fn control(&self) -> Control {
        Control::new(self.ctrl_sender.clone())
    }

    /// Makes progress for Swarm
    /// in general, it should be spawned in a Task
    pub async fn next(&mut self) -> Result<()> {
        // TODO: check if terminated??

        // handle messages, which makes actual progress for Swarm
        self.handle_messages().await?;

        Ok(())
    }

    /// Handles events generated internally or externally
    ///
    /// invoked from `next_stream`
    async fn handle_messages(&mut self) -> Result<()> {
        loop {
            let either = future::select(self.event_receiver.next(), self.ctrl_receiver.next()).await;
            match either {
                Either::Left((evt, _)) => {
                    if let Some(evt) = evt {
                        self.on_event(evt)?;
                    } else {
                        // we are closed anyway, break
                        log::debug!("Swarm event channel is closed, closing down...");
                        return Err(SwarmError::Closing(1));
                    }
                }
                Either::Right((cmd, _)) => {
                    if let Some(cmd) = cmd {
                        self.on_command(cmd).await?;
                    } else {
                        // we are closed anyway, break
                        log::debug!("Swarm control channel is closed, closing down...");
                        return Err(SwarmError::Closing(2));
                    }
                }
            }
        }
    }

    fn on_event(&mut self, event: SwarmEvent) -> Result<()> {
        log::trace!("Swarm event={:?}", event);

        match event {
            SwarmEvent::ListenerClosed { addresses: _, reason: _ } => {}
            SwarmEvent::ConnectionEstablished { stream_muxer, direction } => {
                let _ = self.handle_connection_opened(stream_muxer, direction);
            }
            SwarmEvent::ConnectionClosed { cid, error: _ } => {
                let _ = self.handle_connection_closed(cid);
            }
            SwarmEvent::OutgoingConnectionError {
                peer_id: _,
                remote_addr: _,
                error: _,
            } => {
                // TODO: add statistics
            }
            SwarmEvent::StreamError { .. } => {
                // TODO: add statistics
            }
            SwarmEvent::StreamOpened { sub_stream } => {
                let _ = self.handle_stream_opened(sub_stream);
            }
            SwarmEvent::StreamClosed { cid, sid } => {
                let _ = self.handle_stream_closed(cid, sid);
            }
            SwarmEvent::PingResult { cid, result } => {
                let _ = self.handle_ping_result(cid, result);
            }
            SwarmEvent::IdentifyResult { cid, result } => {
                let _ = self.handle_identify_result(cid, result);
            }

            // TODO: handle other messages
            e => {
                log::warn!("TODO: unhandled swarm events {:?}", e);
            }
        }

        Ok(())
    }

    async fn on_command(&mut self, cmd: SwarmControlCmd) -> Result<()> {
        log::trace!("Swarm control command={:?}", cmd);

        match cmd {
            SwarmControlCmd::NewConnection(peer_id, reply) => {
                // got the peer_id, start the dialer for it
                let _ = self.on_new_connection(peer_id, reply);
            }
            SwarmControlCmd::CloseConnection(peer_id, reply) => {
                // got the peer_id, close all connections to the peer
                let _ = self.on_close_connection(peer_id, reply);
            }
            SwarmControlCmd::NewStream(peer_id, pids, reply) => {
                // got the peer_id, try opening a new sub stream
                let _ = self.on_new_stream(peer_id, pids, reply);
            }
            SwarmControlCmd::CloseStream(cid, sid) => {
                // got the connection_id, try closing a new sub stream
                let _ = self.on_close_stream(cid, sid).await;
            }
            SwarmControlCmd::NetworkInfo(reply) => {
                // Received from channel, try retrieving network info
                let _ = self.on_retrieve_network_info(|r| {
                    let _ = reply.send(r);
                });
            }
            SwarmControlCmd::IdentifyInfo(reply) => {
                // Received from channel, try retrieving identify info
                let _ = self.on_retrieve_identify_info(|r| {
                    let _ = reply.send(r);
                });
            }
            SwarmControlCmd::CloseSwarm => {
                log::info!("closing the swarm...");
                let _ = self.event_sender.close_channel();
            } // TODO:
              //_ => {}
        }

        Ok(())
    }

    fn on_new_connection(&mut self, peer_id: PeerId, reply: oneshot::Sender<Result<()>>) -> Result<()> {
        // return if we already have the connection, otherwise, start dialing
        if let Some(_conn) = self.get_best_conn(&peer_id) {
            let _ = reply.send(Ok(()));
        } else {
            // Note: reply moved
            let _ = self.start_dialer(peer_id, Some(reply));
        }
        Ok(())
    }

    fn on_close_connection(&mut self, peer_id: PeerId, reply: oneshot::Sender<Result<()>>) -> Result<()> {
        if let Some(ids) = self.connections_by_peer.get(&peer_id) {
            // close all connections related to the peer_id
            for id in ids {
                if let Some(c) = self.connections_by_id.get(&id) {
                    c.close()
                }
            }
        }

        let _ = reply.send(Ok(()));
        Ok(())
    }

    fn on_new_stream(&mut self, peer_id: PeerId, pids: Vec<ProtocolId>, reply: oneshot::Sender<Result<Substream>>) -> Result<()> {
        if let Some(connection) = self.get_best_conn(&peer_id) {
            // well, we have a connection, start a task to open the stream
            connection.open_stream(pids, |r| {
                let _ = reply.send(r.map_err(|e| e.into()));
            });
        } else {
            let _ = reply.send(Err(SwarmError::NoConnection(peer_id)));
        }
        Ok(())
    }

    async fn on_close_stream(&mut self, cid: ConnectionId, sid: StreamId) -> Result<()> {
        let _ = self.event_sender.send(SwarmEvent::StreamClosed { cid, sid }).await;
        //self.handle_stream_closed(Direction::Outbound, cid, sid);
        Ok(())
    }
    ///
    fn on_retrieve_network_info(&mut self, f: impl FnOnce(Result<NetworkInfo>)) -> Result<()> {
        f(Ok(self.network_info()));
        Ok(())
    }
    ///
    fn on_retrieve_identify_info(&mut self, f: impl FnOnce(Result<IdentifyInfo>)) -> Result<()> {
        f(Ok(self.identify_info()));
        Ok(())
    }
    /// Starts Swarm background task
    /// handling the internal events and external controls
    pub fn start(self) {
        // well, self 'move' explicitly,
        let mut swarm = self;

        task::spawn(async move { while let Ok(()) = swarm.next().await {} });
    }
    //
    // /// Returns the transport passed when building this object.
    // pub fn transport(&self) -> &TTrans {
    //     &self.transport
    // }
    //
    /// Returns network information about the `Swarm`.
    pub fn network_info(&self) -> NetworkInfo {
        // TODO: add stats later on
        let num_connections_established = self.connections_by_id.len();
        let num_connections_pending = 0; //self.pool.num_pending();
        let num_connections = num_connections_established + num_connections_pending;
        let num_peers = self.connections_by_peer.len();
        let num_active_streams = self.connections_by_id.iter().fold(0, |acc, (_k, v)| acc + v.num_streams());
        let mut connection_info = vec![];
        self.connections_by_id.iter().for_each(|(_id, c)| connection_info.push(c.info()));

        NetworkInfo {
            num_peers,
            num_connections,
            num_connections_established,
            num_connections_pending,
            num_active_streams,
            connection_info,
        }
    }
    /// Returns identify information about the `Swarm`.
    pub fn identify_info(&self) -> IdentifyInfo {
        let protocols = self
            .muxer
            .supported_protocols()
            .into_iter()
            .map(|p| p.protocol_name_str().to_string())
            .collect();

        let public_key = self.peers.keys.get_key(self.local_peer_id()).unwrap().clone();

        IdentifyInfo {
            public_key,
            protocol_version: "".to_string(),
            agent_version: "abc".to_string(),
            listen_addrs: self.listened_addrs.to_vec(),
            protocols,
        }
    }

    /// Starts listening on the given address.
    ///
    /// Returns an error if the address is not supported.
    pub fn listen_on(&mut self, addrs: Vec<Multiaddr>) -> Result<()> {
        let mut succeeded: u32 = 0;
        let mut errs = Vec::new();
        for (i, n) in addrs.clone().into_iter().enumerate() {
            let r = self.add_listen_addr(n);
            match r {
                Ok(_) => succeeded += 1,
                Err(e) => {
                    errs.insert(i, e);
                }
            };
        }

        for (i, n) in errs.into_iter().enumerate() {
            log::warn!("listen on {} failed: {}", addrs[i], n)
        }

        if succeeded == 0 && !addrs.is_empty() {
            log::error!("failed to listen on any addresses:{:?}", addrs);
            return Err(SwarmError::CanNotListenOnAny);
        }

        Ok(())
    }

    /*    /// Remove some listener.
        ///
        /// Returns `Ok(())` if there was a listener with this ID.
        pub fn remove_listener(&mut self, id: ListenerId) -> Result<()> {
            if let Some(i) = self.listeners.iter().position(|l| l.id == id) {
                self.listeners.remove(i);
                Ok(())
            } else {
                Err(SwarmError::Internal)
            }
        }
    */
    fn add_listen_addr(&mut self, addr: Multiaddr) -> Result<()> {
        let mut transport = self.get_best_transport(addr.clone())?;
        let mut listener = transport.listen_on(addr)?;
        self.listened_addrs.push(listener.multi_addr());

        let mut tx = self.event_sender.clone();
        // start a task for this listener
        // TODO: remember the task handle of this listener, so that we can 'cancel' it when exiting
        task::spawn(async move {
            loop {
                let r = listener.accept().await;
                match r {
                    Ok(muxer) => {
                        // don't have to verify if remote peer id matches its public key
                        // always accept any incoming connection
                        // send muxer back to Swarm main task
                        let _ = tx
                            .send(SwarmEvent::ConnectionEstablished {
                                stream_muxer: muxer,
                                direction: Direction::Inbound,
                            })
                            .await;
                    }
                    Err(err) => {
                        let _ = tx
                            .send(SwarmEvent::IncomingConnectionError {
                                // TODO:
                                remote_addr: Multiaddr::empty(),
                                error: err,
                            })
                            .await;
                    }
                }
            }
        });
        Ok(())
    }
    ///  retrieves the appropriate transport for listening on or dial the given multiaddr.
    fn get_best_transport(&self, mut addr: Multiaddr) -> Result<ITransportEx> {
        let protocol = addr.pop();
        match protocol {
            Some(d) => {
                log::info!("get best transport, protocol={}", &d);
                let id = d
                    .get_key()
                    .map_err(|_| SwarmError::Transport(TransportError::MultiaddrNotSupported(addr)))?;
                if let Some(selected) = self.transports.get(&id).map(|s| s.box_clone()) {
                    return Ok(selected);
                }
                Err(SwarmError::TransportsNotRegistered)
            }
            None => Err(SwarmError::Transport(TransportError::MultiaddrNotSupported(addr))),
        }
    }
    /// Tries to dial the given address.
    ///
    /// Returns an error if the address is not supported.
    fn dial_peer_with_addr(&mut self, peer_id: PeerId, addr: Multiaddr, reply: Option<oneshot::Sender<Result<()>>>) {
        // TODO: add dial limiter...

        log::trace!("dialing addr={:?}, expecting {:?}", addr, peer_id);

        let mut tx = self.event_sender.clone();
        // TODO: first_mut ==> per protocol
        //let mut transport = self.transports.first_mut().unwrap().box_clone();
        let mut transport = self.get_best_transport(addr.clone()).unwrap();
        task::spawn(async move {
            let r = transport.dial(addr.clone()).await;
            let response = match r {
                Ok(stream_muxer) => {
                    // test if the PeerId matches expectation, otherwise,
                    // it is a bad outgoing connection
                    if peer_id == stream_muxer.remote_peer() {
                        let _ = tx
                            .send(SwarmEvent::ConnectionEstablished {
                                stream_muxer,
                                direction: Direction::Outbound,
                            })
                            .await;
                        Ok(())
                    } else {
                        let wrong_id = stream_muxer.remote_peer();
                        log::info!(
                            "bad connection, peerid mismatch conn={:?} wanted={:?} got={:?}",
                            stream_muxer,
                            peer_id,
                            wrong_id
                        );
                        let _ = tx
                            .send(SwarmEvent::OutgoingConnectionError {
                                peer_id,
                                remote_addr: addr,
                                error: TransportError::Internal,
                            })
                            .await;

                        Err(SwarmError::InvalidPeerId(wrong_id))
                    }
                }
                Err(err) => {
                    let _ = tx
                        .send(SwarmEvent::OutgoingConnectionError {
                            peer_id,
                            remote_addr: addr,
                            error: TransportError::Internal,
                        })
                        .await;

                    Err(SwarmError::Transport(err))
                }
            };
            reply.map(|reply| reply.send(response));
        });
    }

    /// Starts a dialing task
    /// reply is optional, it might be 'None' when dialer is initiated internally
    pub fn start_dialer(&mut self, peer_id: PeerId, reply: Option<oneshot::Sender<Result<()>>>) -> Result<()> {
        log::trace!("dialer, looking for {:?}", peer_id);

        // TODO: find a better way to handle multiple addresses of PeerId
        if let Some(addrs) = self.peers.addrs.get_addr(&peer_id) {
            // TODO: handle multiple addresses

            // TODO: add dial limiter...

            let addr = addrs.first().expect("must have one").clone();
            self.dial_peer_with_addr(peer_id, addr.addr, reply);

            Ok(())
        } else {
            Err(SwarmError::NoAddresses(peer_id))
        }
    }

    /// Tries to initiate a dialing attempt to the given peer.
    ///
    pub async fn dial_peer(&mut self, peer_id: PeerId) -> Result<()> {
        if let Some(_conn) = self.get_best_conn(&peer_id) {
            Ok(())
        } else {
            self.start_dialer(peer_id, None)
        }
    }

    fn get_best_conn(&mut self, peer_id: &PeerId) -> Option<&mut Connection> {
        let mut best = None;
        // selects the best connection we have to the peer.
        // TODO: we might have multiple connections towards a PeerId

        log::trace!("trying to get the best connnection for {:?}", peer_id);

        self.connections_by_peer.iter().for_each(|(k, v)| log::info!("{:?}={:?}", k, v));

        //let v = self.connections_by_peer.get_mut(peer_id).unwrap();

        if let Some(ids) = self.connections_by_peer.get(peer_id) {
            // TODO: to check if this connection is being closed

            let mut len = 0;
            for id in ids.iter() {
                if let Some(connection) = self.connections_by_id.get(id) {
                    let num = connection.num_streams();
                    if num >= len {
                        len = num;
                        best = Some(id);
                    }
                }
            }
        }

        if let Some(id) = best {
            self.connections_by_id.get_mut(id)
        } else {
            None
        }
    }

    fn is_connected(&self, peer_id: &PeerId) -> bool {
        // TODO: check if the connection is being closed??
        self.connections_by_peer.get(peer_id).map_or(0, |v| v.len()) > 0
    }

    /*
        /// Tries to initiate a dialing attempt to the given peer.
        ///
        /// If a new dialing attempt has been initiated, `Ok(true)` is returned.
        ///
        /// If no new dialing attempt has been initiated, meaning there is an ongoing
        /// dialing attempt or `addresses_of_peer` reports no addresses, `Ok(false)`
        /// is returned.
        pub fn dial(&mut self, peer_id: &PeerId) -> Result<(), SwarmError> {
            let self_listening = &self.listened_addrs;
            let mut addrs = self.behaviour.addresses_of_peer(peer_id)
                .into_iter()
                .filter(|a| !self_listening.contains(a));

            let result =
                if let Some(first) = addrs.next() {
                    let handler = self.behaviour.new_handler().into_node_handler_builder();
                    self.network.peer(peer_id.clone())
                        .dial(first, addrs, handler)
                        .map(|_| ())
                        .map_err(SwarmError::ConnectionLimit)
                } else {
                    Err(SwarmError::NoAddresses)
                };

            if let Err(error) = &result {
                log::debug!(
                    "New dialing attempt to peer {:?} failed: {:?}.",
                    peer_id, error);
                self.behaviour.inject_dial_failure(&peer_id);
            }

            result
        }
        /// Returns an iterator that produces the list of addresses we're listening on.
        pub fn listeners(&self) -> impl Iterator<Item = &Multiaddr> {
            self.listeners.iter().flat_map(|l| l.addresses.iter())
        }
    */
    /*
        /// Set a handler for sub streams, with a protocol id
        ///
        /// , f: F)
        //     where F: FnMut(TStreamMuxer
    ::Substream
        fn set_stream_handler<F>(&mut self, pid: TProto)
        {
            self.muxer.add_handler(pid, Box::new(f));
        }*/

    /// Returns an iterator that produces the list of addresses that other nodes can use to reach
    /// us.
    pub fn external_addresses(&self) -> impl Iterator<Item = &Multiaddr> {
        self.external_addrs.iter()
    }

    /// Returns the peer ID of the swarm passed as parameter.
    pub fn local_peer_id(&self) -> &PeerId {
        &self.local_peer_id
    }

    /// Adds an external address.
    ///
    /// An external address is an address we are listening on but that accounts for things such as
    /// NAT traversal.
    pub fn add_external_address(&mut self, addr: Multiaddr) {
        self.external_addrs.add(addr)
    }
    /*
        /// Obtains a view of a [`Peer`] with the given ID in the network.
        pub fn peer(&mut self, peer_id: TPeerId)
                    -> Peer<'_, TTrans, TInEvent, TOutEvent, THandler, TConnInfo, TPeerId>
        {
            Peer::new(self, peer_id)
        }

        /// Returns the connection info for an arbitrary connection with the peer, or `None`
        /// if there is no connection to that peer.
        // TODO: should take &self instead of &mut self, but the API in network requires &mut
        pub fn connection_info(&mut self, peer_id: &PeerId) -> Option<TConnInfo> {
            if let Some(mut n) = self.network.peer(peer_id.clone()).into_connected() {
                Some(n.some_connection().info().clone())
            } else {
                None
            }
        }
    */
    /// Bans a peer by its peer ID.
    ///
    /// Any incoming connection and any dialing attempt will immediately be rejected.
    /// This function has no effect is the peer is already banned.
    pub fn ban_peer_id(&mut self, peer_id: PeerId) {
        self.banned_peers.insert(peer_id);

        // TODO: to disconnect
        // if let Some(c) = self.network.peer(peer_id).into_connected() {
        //     c.disconnect();
        // }
    }

    /// Unbans a peer.
    pub fn unban_peer_id(&mut self, peer_id: PeerId) {
        self.banned_peers.remove(peer_id.as_ref());
    }

    fn add_connection(&mut self, connection: Connection) {
        let cid = connection.id();
        let remote_peer_id = connection.remote_peer();

        // append to the by_id hashmap
        self.connections_by_id.insert(cid, connection);

        // append to the by peer hashmap
        let conns = self.connections_by_peer.entry(remote_peer_id).or_default();
        conns.push(cid);

        log::trace!("connection added to hashmap, total={}", self.connections_by_id.len());

        // TODO: we have a connection to the specified peer_id, now cancel all pending attempts

        // TODO: generate a connected event

        // TODO: return the connection

        // start Ping service if need
    }

    /// Handles a new connection
    ///
    /// start a Task for accepting new sub-stream from the connection
    fn handle_connection_opened(&mut self, stream_muxer: IStreamMuxer, dir: Direction) -> Result<()> {
        log::trace!("handle_connection_opened: {:?} {:?}", stream_muxer, dir);

        let pid = self.local_peer_id.clone();
        let pubkey = stream_muxer.clone().local_priv_key().public();

        self.peers.keys.add_key(&pid, pubkey);

        // clone the stream_muxer, and then wrap into Connection, task_handle will be assigned later
        let mut connection = Connection::new(
            self.assign_cid(),
            stream_muxer.box_clone(),
            dir,
            self.event_sender.clone(),
            self.ctrl_sender.clone(),
        );

        // TODO: filtering the multiaddr, Err = AddrFiltered(addr)

        /*        raddr := tc.RemoteMultiaddr()
                if s.Filters.AddrBlocked(raddr) {
                    tc.Close()
                    return nil, ErrAddrFiltered
                }

                p := tc.RemotePeer()

                // Add the public key.
                if pk := tc.RemotePublicKey(); pk != nil {
                    s.peers.AddPubKey(p, pk)
                }
        */
        // TODO: add remote pubkey to keystore

        let mut tx = self.event_sender.clone();
        let cid = connection.id();
        // clone muxer and move it into the task
        let mut muxer = self.muxer.clone();
        let ctrl = self.ctrl_sender.clone();

        // Note we have to use the original copy of the stream muxer to start the task,
        // instead of the cloned one which doesn't have the task handle at all
        let handle = task::spawn(async move {
            let mut stream_muxer = stream_muxer;
            // start the background task of the stream_muxer, the handle can be await'ed by us
            let task_handle = stream_muxer.task().map(task::spawn);
            loop {
                let ctrl = ctrl.clone();
                let r = stream_muxer.accept_stream().await;

                // TODO: probably we should spawn a new task for protocol selection
                // But in go-libp2p, the protocol selection is done in a blocking way...

                match r {
                    Ok(raw_stream) => {
                        // well, got the raw stream, now do protocol selection
                        log::trace!("run protocol selection for inbound stream={:?}", raw_stream);

                        // now it's time to do multistream multiplexing for inbound stream
                        let result = muxer.select_inbound(raw_stream).await;
                        match result {
                            Ok((mut handler, raw_stream, proto)) => {
                                let la = stream_muxer.local_multiaddr();
                                let ra = stream_muxer.remote_multiaddr();
                                let stream = Substream::new(raw_stream, Direction::Inbound, proto, cid, la, ra, ctrl);
                                let sub_stream = stream.clone();
                                let _ = tx.send(SwarmEvent::StreamOpened { sub_stream }).await;

                                // anyway, start handler task
                                task::spawn(async move {
                                    let _ = handler.handle(stream, proto).await;
                                });

                                // TODO: hook the task handle to the Substream, so that it can wait for exiting the task
                            }
                            Err(error) => {
                                log::debug!("failed inbound protocol selection {:?} {:?}", cid, error);
                                let _ = tx.send(SwarmEvent::StreamError { cid, error }).await;
                            }
                        }
                    }
                    Err(error) => {
                        log::debug!("connection closed {:?} {:?}", cid, error);
                        let _ = tx.send(SwarmEvent::ConnectionClosed { cid, error }).await;
                        // something happened, break the loop then exit the Task
                        break;
                    }
                }
            }

            // As stream_muxer is closed, we wait for its task_handle
            if let Some(h) = task_handle {
                h.await;
            }

            log::trace!("accept-task exiting...");
        });

        // now we have the handle, move it into Connection
        connection.set_handle(handle);

        // start Ping service if there is
        if let Some(config) = self.ping.as_ref() {
            if config.unsolicited() {
                log::trace!("starting Ping service for {:?}", connection);
                connection.start_ping(config.timeout(), config.interval());
            }
        };

        // start Identify service if there is
        if let Some(_config) = self.identify.as_ref() {
            log::trace!("starting Identify service for {:?}", connection);
            connection.start_identify();
        };

        // start Identify push service if there is
        if let Some(config) = self.identify.as_ref() {
            if config.push {
                log::trace!("starting Identify Push service for {:?}", connection);
                let pubkey = self.peers.keys.get_key(&connection.local_peer()).unwrap().clone();
                connection.start_identify_push(pubkey);
            }
        };

        // insert to the hashmap of connections
        // there might be a race condition:
        // the spawned connection task might have exited for some reason, before we insert connection
        // into the hashmap. No problem, the connection will be cleaned up in 'handle_connection_closed'
        // event.
        self.add_connection(connection);

        Ok(())
    }

    /// Handles opening stream
    ///
    /// Use channel to received message that sent by another task
    fn handle_stream_opened(&mut self, sub_stream: Substream) -> Result<()> {
        log::trace!("handle_stream_opened: {:?}", sub_stream);
        let cid = sub_stream.cid();
        // add stream id to the connection substream list
        if let Some(c) = self.connections_by_id.get_mut(&cid) {
            c.add_stream(sub_stream)
        };
        Ok(())
    }

    /// Handles closing stream
    fn handle_stream_closed(&mut self, cid: ConnectionId, sid: StreamId) -> Result<()> {
        log::trace!("handle_stream_closed: {:?}/{:?}", cid, sid);
        // delete sub-stream from the connection substream list
        if let Some(c) = self.connections_by_id.get_mut(&cid) {
            c.del_stream(sid)
        }
        Ok(())
    }

    /// Handles closing a connection
    ///
    /// start a Task for accepting new sub-stream from the connection
    fn handle_connection_closed(&mut self, cid: ConnectionId) -> Result<()> {
        log::info!("before close {:?}", self.connections_by_id);
        log::info!("before close {:?}", self.connections_by_peer);

        log::trace!("handle_connection_closed: {:?}", cid);

        // try to retrieve the Connection by looking up 'connections_by_id'
        if let Some(mut connection) = self.connections_by_id.remove(&cid) {
            let remote_peer_id = connection.remote_peer();
            if let Some(ids) = self.connections_by_peer.get_mut(&remote_peer_id) {
                ids.retain(|id| id != &cid);
            } else {
                log::warn!("shouldn't happen, PeerId={:?}", remote_peer_id);
            }

            // now, close all tasks owned by the connection
            task::spawn(async move {
                let _ = connection.wait().await;
                let _ = connection.stop_ping().await;
                let _ = connection.stop_identify().await;
                let _ = connection.stop_identify_push().await;
            });
        } else {
            log::info!("shouldn't happen, wired connection {:?}", cid);
        }

        log::info!("after close {:?}", self.connections_by_id);
        log::info!("after close {:?}", self.connections_by_peer);

        Ok(())
    }

    /// Received ping result, and then updates addrbook in peerstore
    fn handle_ping_result(&mut self, cid: ConnectionId, result: Result<Duration>) -> Result<()> {
        log::trace!("handle_ping_result: {:?} {:?}", cid, result);

        if let Some(connection) = self.connections_by_id.get_mut(&cid) {
            match result {
                Ok(ttl) => {
                    //let remote_peer_id = c.remote_peer();
                    log::trace!("ping TTL={:?} for {:?}", ttl, connection);
                    // TODO: update peer store with the TTL
                    let peer_id = connection.stream_muxer().remote_peer();
                    self.peers.addrs.update_addr(&peer_id, Duration::from_secs(1), ttl);

                    connection.reset_failure();
                }
                Err(err) => {
                    log::info!("ping failed {:?} for {:?}", err, connection);
                    let allowed_max_failure = self.ping.as_ref().map_or(0, |config| config.max_failures());

                    connection.handle_failure(allowed_max_failure);
                }
            }
        }

        Ok(())
    }

    /// Received result which contains IdentityInfo and multiaddr,
    /// and then updates keybook and protobook in peerstore.
    fn handle_identify_result(&mut self, cid: ConnectionId, result: Result<(IdentifyInfo, Multiaddr)>) -> Result<()> {
        log::trace!("handle_identify_result: {:?}", cid);

        if let Some(connection) = self.connections_by_id.get_mut(&cid) {
            match result {
                Ok((info, observed_addr)) => {
                    let peer_id = connection.stream_muxer().remote_peer();
                    //let remote_peer_id = c.remote_peer();
                    log::trace!("identify observed_addr: {} info={:?} for {:?}", observed_addr, info, connection);

                    // TODO: update peer store with the
                    self.peers.addrs.add_addr(&peer_id, observed_addr, Duration::from_secs(1));
                    self.peers.protos.add_protocol(&peer_id, info.protocols);
                    //
                }
                Err(err) => {
                    log::info!("identify failed {:?} for {:?}", err, connection);
                }
            }
        }

        Ok(())
    }
}

/// The possible failures of [`Swarm`].
#[derive(Debug)]
pub enum SwarmError {
    /// The configured limit for simultaneous outgoing connections
    /// has been reached.
    ConnectionLimit(ConnectionLimit),
    /// Returned no addresses for the peer to dial.
    NoAddresses(PeerId),
    /// No connection yet, unable to open a sub stream.
    NoConnection(PeerId),
    /// The peer identity obtained on the connection did not
    /// match the one that was expected.
    InvalidPeerId(PeerId),

    /// Closing down. Swarm is being closed at this moment
    /// 1: event channel, 2: ctrl channel
    Closing(u32),

    /// Transport Error
    ///
    /// Contains a TransportError.
    Transport(TransportError),

    /// Internal, tentatively for convenience
    Internal,

    /// listen on fail
    CanNotListenOnAny,

    /// Transports not registered
    TransportsNotRegistered,
}

impl fmt::Display for SwarmError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            SwarmError::ConnectionLimit(err) => write!(f, "Swarm Dial error: {}", err),
            SwarmError::NoAddresses(peer_id) => write!(f, "Swarm Dial error: no addresses for peer{:?}.", peer_id),
            SwarmError::NoConnection(peer_id) => write!(f, "Swarm Stream error: no connections for peer{:?}.", peer_id),
            SwarmError::InvalidPeerId(peer_id) => write!(f, "Swarm Dial error: invalid peer id{:?}.", peer_id),
            SwarmError::Transport(err) => write!(f, "Swarm Transport error: {}.", err),
            SwarmError::Internal => write!(f, "Swarm internal error."),
            SwarmError::Closing(s) => write!(f, "Swarm channel closed source={}.", s),
            SwarmError::CanNotListenOnAny => write!(f, "Failed to listen on any addresses"),
            SwarmError::TransportsNotRegistered => write!(f, "Transports not registered"),
        }
    }
}

impl error::Error for SwarmError {
    fn source(&self) -> Option<&(dyn error::Error + 'static)> {
        match self {
            SwarmError::ConnectionLimit(err) => Some(err),
            SwarmError::NoAddresses(_) => None,
            SwarmError::NoConnection(_) => None,
            SwarmError::InvalidPeerId(_) => None,
            SwarmError::Transport(err) => Some(err),
            SwarmError::Internal => None,
            SwarmError::Closing(_) => None,
            SwarmError::CanNotListenOnAny => None,
            SwarmError::TransportsNotRegistered => None,
        }
    }
}

impl From<std::io::Error> for SwarmError {
    fn from(err: std::io::Error) -> Self {
        SwarmError::Transport(TransportError::IoError(err))
    }
}

impl From<TransportError> for SwarmError {
    fn from(err: TransportError) -> Self {
        SwarmError::Transport(err)
    }
}

impl From<mpsc::SendError> for SwarmError {
    // TODO: make a error catelog for SendError
    fn from(_: mpsc::SendError) -> Self {
        SwarmError::Internal
    }
}

impl From<oneshot::Canceled> for SwarmError {
    // TODO: make a error catelog for Canceled
    fn from(_: oneshot::Canceled) -> Self {
        SwarmError::Internal
    }
}

#[cfg(test)]
mod tests {}
